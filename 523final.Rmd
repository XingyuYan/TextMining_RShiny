---
title: "Final Project"
author: Text Miner - Group 9
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## install necessary packages
# install.packages("wordcloud")
# install.packages("cluster")

```

## Text mining

The text mining part is to input the file, to clean it up and to output a table where each word and its appearing frequency are listed. The cleaning method is using both stringr and gsub, where we get rid of all punctuations, numbers, simple letters, and small words which have less than four letters in it. Besides, as we don't want the stop words, such as "I", "and", and "the", we delete them as well. At the end, we output a dataframe which contains two columns: keys and values.

```{r}
  # load packages
  library(purrr)
  library(magrittr)
  library(stringr)
  library(dplyr)
  library(tm)
  library(SnowballC)   


  data = readLines("texts.txt") %>% as.list()
  
  # Map step
  clean_text = function(val)
  {
    # delete punctuations and numbers
    val = gsub("[[:punct:]]", " ", val)
    val = gsub("[[:digit:]]+", " ", val)
    # clean it up
    clean_a =  val %>%
      tolower() %>%
      str_split(" ") 
    
    delete_single = function(x){
      if(length(x) != 1){
        x = x[-which(sapply(x,nchar) == 1)]
      }
      return(x)
    }
    
    clean_a = lapply(clean_a,function(x) x[x  != "" & ! x %in% stopwords("en")])
    clean_a = lapply(clean_a,delete_single)
    clean_a = lapply(clean_a, function(x) paste(x,collapse = " "))
    clean_a = clean_a[-which(clean_a == "")]
   return(clean_a)
  }
  
  a = clean_text(data)
  
   #In many cases, words need to be stemmed to retrieve their radicals. For instance, "example" and "examples" are both stemmed to "exampl". However, after that, one may want to complete the stems to their original forms, so that the words would look "normal".
  text = Corpus(VectorSource(a))
  text = tm_map(text, removeWords, stopwords("english"))
  text = tm_map(text,stemDocument)  
  text = tm_map(text, stripWhitespace) 
  text = tm_map(text, PlainTextDocument) 
  myDtm = TermDocumentMatrix(text, control = list(minWordLength = 1))
   
  # find frequency: 
  freq = colSums(t(as.matrix(myDtm)))
  # order   
  ord = rev(order(freq))
  
  # top n freq word 
  n = 10
  freq[ord[1:n]]

  # output df
  wf = data.frame(word=names(freq), freq=freq) 
  wf = wf %>% group_by(word) %>% arrange(desc(freq))
  

```

## Displaying

```{r}

  library(ggplot2)
  
  #The n used most word histogram
  n = 10
  hist_data = wf[1:n,]
  ggplot(hist_data, aes(word, freq))+
    geom_bar(stat="identity") +
    theme(axis.text.x=element_text(angle=45, hjust=1)) +
    theme_minimal() + 
    ylab("Frequency") +
    xlab("")
     

  # the n most used word 
  n = 10
  wordcloud(names(freq), freq, max.words=100, rot.per=0.2, 
          colors=brewer.pal(6, "Dark2") )   

  ## Figure 3 
  library(cluster)   
  # remove sparse terms
  dtmss = removeSparseTerms(myDtm, 0.9) 
  d = dist((dtmss), method="euclidian")
  fit = hclust(d=d, method="ward.D")   
  plot(fit, hang=-1)   
  
  rect.hclust(fit, k=5, border="red") # draw dendogram with red borders around the 5 clusters   
  groups = cutree(fit, k=5)   # "k=" defines the number of clusters you are using   
  
  ## Figure 4
  kfit = kmeans(d, 5)   
  clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
  
  # figure 5
  library(igraph)
  termDocMatrix = as.matrix(dtmss)
  termDocMatrix[termDocMatrix>=1] = 1
  termMatrix = termDocMatrix %*% t(termDocMatrix)
  g = graph.adjacency(termMatrix, weighted=T, mode = "undirected")
  V(g)$label = V(g)$name
  V(g)$degree = degree(g)
  # remove loops
  g = simplify(g)
  V(g)$label.cex = log(rank(V(g)$degree)) + 1
  V(g)$label.color = rgb(0, 0, .2, .8)
  V(g)$frame.color = NA
  egam = (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)
  E(g)$color = rgb(.5, .5, 0, egam)
  E(g)$width = egam*2

  plot(g)

```


## Shiny

```{r}


```